{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:31.613308Z",
     "start_time": "2019-07-26T02:19:30.702562Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:31.729817Z",
     "start_time": "2019-07-26T02:19:31.615128Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Load data to X,y and initial something useful\n",
    "\n",
    "# 10 labels, from 1 to 10 (note that we have mapped \"0\" to label 10)\n",
    "num_labels = 10\n",
    "\n",
    "#  training data stored in arrays X, y\n",
    "# every row of X is a training sample\n",
    "# data form X(5000,400),y(50,)\n",
    "data_path = 'C:\\\\Users\\\\39410\\\\Desktop\\\\大创项目\\\\吴恩达机器学习算法数据\\\\machine-learning-ex3\\\\ex3\\\\ex3data1.mat'\n",
    "data = loadmat(data_path)\n",
    "X, y = data['X'], data['y'].ravel()\n",
    "\n",
    "# set the zero digit to 0, rather than its mapped 10 in this dataset\n",
    "# This is an artifact due to the fact that this dataset was used in\n",
    "# MATLAB where there is no index 0\n",
    "y[y == 10] = 0\n",
    "\n",
    "# number of samples\n",
    "m = y.size\n",
    "\n",
    "input_layer_size = 400  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25  # 25 hidden units \n",
    "# Note: These doesn't include bias term\n",
    "num_labels = 10  # 10 labels, from 0 to 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:31.758739Z",
     "start_time": "2019-07-26T02:19:31.732811Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Load the pretraining parameters for this exercise\n",
    "\n",
    "# Load the .mat file, which returns a dictionary \n",
    "weights_path = 'C://Users//39410//Desktop//大创项目//吴恩达机器学习算法数据//machine-learning-ex3//ex3//ex3weights.mat'\n",
    "weights = loadmat(weights_path)\n",
    "\n",
    "# get the model weights from the dictionary\n",
    "# Theta1 has size 25 x 401 # theta of the first layer, there are 401 input units,25 hidden units,10 output units\n",
    "# Theta2 has size 10 x 26 # theta of the second layer, \n",
    "Theta1, Theta2 = weights['Theta1'], weights['Theta2']\n",
    "\n",
    "# swap first and last columns of Theta2, due to legacy from MATLAB indexing, \n",
    "# since the weight file ex3weights.mat was saved based on MATLAB indexing\n",
    "Theta2 = np.roll(Theta2, 1, axis=0)\n",
    "\n",
    "# Unroll parameters \n",
    "nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()]) \n",
    "#将Theta1和Theta2统一成一个向量\n",
    "# print(type(nn_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:31.772702Z",
     "start_time": "2019-07-26T02:19:31.763729Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "# 可以计算矩阵，也可以计算标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:31.788660Z",
     "start_time": "2019-07-26T02:19:31.779684Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    return np.multiply(sigmoid(z),(1.0 - sigmoid(z)))\n",
    "# 可以计算矩阵，也可以计算标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:31.799630Z",
     "start_time": "2019-07-26T02:19:31.793647Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # Test sigmoidGrandient\n",
    "# z = np.array([-1, -0.5, 0, 0.5, 1])\n",
    "# g = sigmoidGradient(z)\n",
    "# print('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ')\n",
    "# print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:31.814590Z",
     "start_time": "2019-07-26T02:19:31.804617Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test Python Block\n",
    "# x1 = np.zeros((5,6))\n",
    "# x2 = np.array([1,2,3,4,5,6]).reshape(2,3)\n",
    "# x2[:,1:] += 1\n",
    "# print(x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize weights and forwardPass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:31.839524Z",
     "start_time": "2019-07-26T02:19:31.822569Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out, epsilon_init=0.12):\n",
    "    W = epsilon_init * (np.random.rand(L_out,L_in+1) * 2 - 1)\n",
    "    # generate random number in [-epsilon_init,epsilon_init) \n",
    "    # with shape of L_out*(L_in+1)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:31.862463Z",
     "start_time": "2019-07-26T02:19:31.843515Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Neural Network Parameters ...\n"
     ]
    }
   ],
   "source": [
    "# Call randInitializeWeights\n",
    "print('Initializing Neural Network Parameters ...')\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "# Unroll parameters\n",
    "initial_nn_params = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:31.904351Z",
     "start_time": "2019-07-26T02:19:31.868448Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def forwardPass(Theta1,Theta2,X):\n",
    "    if X.ndim == 1:\n",
    "        X = X[None]#将一维的X提升到二维\n",
    "        \n",
    "    m = X.shape[0]\n",
    "    num_labels = Theta2.shape[0]\n",
    "    \n",
    "    z2 = np.dot(np.concatenate([np.ones((m, 1)), X], axis = 1),Theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(np.concatenate([np.ones((m,1)),a2],axis = 1),Theta2.T)\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    #print(z2.shape)#(5000,25)\n",
    "    #print(a2.shape)#(5000,25)\n",
    "    #print(z3.shape)#(5000,25)\n",
    "    #print(a3.shape)#(5000,10)\n",
    "    # return the activation values and z values of hidden layer and output layer\n",
    "    # for BP training\n",
    "    return z2,a2,z3,a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BP Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:32.081878Z",
     "start_time": "2019-07-26T02:19:31.908339Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,#将Theta1和Theta2统一起来\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X,\n",
    "                   y,\n",
    "                   lambda_=0.0):\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "\n",
    "    # Setup some useful variables\n",
    "    m = y.size\n",
    "    z2,a2,z3,h = forwardPass(Theta1, Theta2, X)     \n",
    "    \n",
    "    # Theta1_grad & Theta2_grad is the gradient of Theta\n",
    "    J = 0\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    # transform y to logic array\n",
    "    temp = np.zeros((m,num_labels))\n",
    "    for i in range(m): \n",
    "        temp[i][y[i]] = 1\n",
    "    y = temp# y.shape = (5000,10)\n",
    "    \n",
    "    # compute J\n",
    "    for i in range(m):\n",
    "        J += np.dot(np.log(h[i]),y[i]) + np.dot(np.log(1-h[i]),(1-y[i]))\n",
    "    J /= (-m)\n",
    "    J += (lambda_/(2*m))*(np.sum(Theta2[:,1:]**2)+np.sum(Theta1[:,1:]**2))#正则化J\n",
    "    # np.log(x)是对整个x矩阵求log，x的每个值都不能为0，\n",
    "    # 1.可以过滤掉为0的数据\n",
    "    # 2.可以添加一个很小的值如epsilon = 1e-5\n",
    "    # bias 单元不正则化里(即Theta1和Theta2的第一列)\n",
    "    \n",
    "    # compute grad\n",
    "    delta3 = h - y\n",
    "    # delta3.shape = (5000,10)\n",
    "\n",
    "    delta2 = np.dot(delta3,Theta2) \n",
    "    delta2 = delta2[:,1:]# ignore the bias term \n",
    "    delta2 = np.multiply(delta2,sigmoidGradient(z2))\n",
    "    #note that np.multiply means element wise\n",
    "    #delta2.shape = (5000,25)\n",
    "    \n",
    "    a1 = np.concatenate([np.ones((m, 1)), X], axis = 1) #a1.shape = (5000,401)\n",
    "    a2 = np.concatenate([np.ones((m,1)),a2],axis = 1)#a2.shape = (5000,26)\n",
    "    Theta1_grad = np.dot(delta2.T,a1) / m\n",
    "    Theta2_grad = np.dot(delta3.T,a2) / m\n",
    "    #print(Theta1_grad.shape) #Theta1.shape = (25,401)\n",
    "    #print(Theta2_grad.shape) #Theta2.shape = (10,26)\n",
    "    Theta1_grad[:,1:] += (lambda_/m) * Theta1[:,1:]\n",
    "    Theta2_grad[:,1:] += (lambda_/m) * Theta2[:,1:]\n",
    "    #note Theta_grad[:,1:] & Theta[:,1:] should have the same shape\n",
    "    \n",
    "    # Unroll gradients\n",
    "    # grad = np.concatenate([Theta1_grad.ravel(order=order), Theta2_grad.ravel(order=order)])\n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:32.186596Z",
     "start_time": "2019-07-26T02:19:32.085869Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights): 0.287629 \n",
      "The cost should be about                   : 0.287629.\n"
     ]
    }
   ],
   "source": [
    "# Test unregular J\n",
    "# Note that this test should use the pretraining weights and be order executed\n",
    "lambda_ = 0\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                   num_labels, X, y, lambda_)\n",
    "print('Cost at parameters (loaded from ex4weights): %.6f ' % J)\n",
    "print('The cost should be about                   : 0.287629.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:32.292313Z",
     "start_time": "2019-07-26T02:19:32.189591Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights): 0.383770\n",
      "This value should be about                 : 0.383770.\n"
     ]
    }
   ],
   "source": [
    "# Test Weight regularization parameter (we set this to 1 here).\n",
    "# Note that this test should use the pretraining weights and be order executed\n",
    "lambda_ = 1\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                      num_labels, X, y, lambda_)\n",
    "\n",
    "print('Cost at parameters (loaded from ex4weights): %.6f' % J)\n",
    "print('This value should be about                 : 0.383770.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:44:53.670278Z",
     "start_time": "2019-07-26T02:44:53.657252Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def debugInitializeWeights(fan_out, fan_in):\n",
    "    \"\"\"\n",
    "    Initialize the weights of a layer with fan_in incoming connections and fan_out outgoings\n",
    "    connections using a fixed strategy. This will help you later in debugging.\n",
    "    Note that W should be set a matrix of size (1+fan_in, fan_out) as the first row of W handles\n",
    "    the \"bias\" terms.\n",
    "    Parameters\n",
    "    ----------\n",
    "    fan_out : int\n",
    "        The number of outgoing connections.\n",
    "    fan_in : int\n",
    "        The number of incoming connections.\n",
    "    Returns\n",
    "    -------\n",
    "    W : array_like (1+fan_in, fan_out)\n",
    "        The initialized weights array given the dimensions.\n",
    "    \"\"\"\n",
    "    # Initialize W using \"sin\". This ensures that W is always of the same values and will be\n",
    "    # useful for debugging\n",
    "    W = np.sin(np.arange(1, 1 + (1+fan_in)*fan_out))/10.0\n",
    "    W = W.reshape(fan_out, 1+fan_in, order='F')\n",
    "    #print(W)\n",
    "    #print(\"---------\")\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:39:49.093903Z",
     "start_time": "2019-07-26T02:39:49.074952Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def computeNumericalGradient(J, theta, e=1e-4):\n",
    "    \"\"\"\n",
    "    Computes the gradient using \"finite differences\" and gives us a numerical estimate of the\n",
    "    gradient.\n",
    "    Parameters\n",
    "    ----------\n",
    "    J : func\n",
    "        The cost function which will be used to estimate its numerical gradient.\n",
    "    theta : array_like\n",
    "        The one dimensional unrolled network parameters. The numerical gradient is computed at\n",
    "         those given parameters.\n",
    "    e : float (optional)\n",
    "        The value to use for epsilon for computing the finite difference.\n",
    "    Notes\n",
    "    -----\n",
    "    The following code implements numerical gradient checking, and\n",
    "    returns the numerical gradient. It sets `numgrad[i]` to (a numerical\n",
    "    approximation of) the partial derivative of J with respect to the\n",
    "    i-th input argument, evaluated at theta. (i.e., `numgrad[i]` should\n",
    "    be the (approximately) the partial derivative of J with respect\n",
    "    to theta[i].)\n",
    "    \"\"\"\n",
    "    numgrad = np.zeros(theta.shape)\n",
    "    perturb = np.diag(e * np.ones(theta.shape)) \n",
    "    # use np.diag to transform (38,) to (38,38) \n",
    "    # and perturb is a diagonal matrix with every digonal is e\n",
    "    # perturb.shape = (38,38)\n",
    "    #print(perturb)\n",
    "    for i in range(theta.size):\n",
    "        loss1, _ = J(theta - perturb[:, i])\n",
    "        loss2, _ = J(theta + perturb[:, i])\n",
    "        numgrad[i] = (loss2 - loss1)/(2*e)\n",
    "    # return the approximation partial derivative of each theta\n",
    "    # numgrad.shape = (38,)\n",
    "    # number of (theta1 + theta2) = 38\n",
    "    return numgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:34:36.863722Z",
     "start_time": "2019-07-26T02:34:36.764983Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def checkNNGradients(nnCostFunction, lambda_=0):\n",
    "    \"\"\"\n",
    "    Creates a small neural network to check the backpropagation gradients. It will output the\n",
    "    analytical gradients produced by your backprop code and the numerical gradients\n",
    "    (computed using computeNumericalGradient). These two gradient computations should result in\n",
    "    very similar values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    nnCostFunction : func\n",
    "        A reference to the cost function implemented by the student.\n",
    "    lambda_ : float (optional)\n",
    "        The regularization parameter value.\n",
    "    \"\"\"\n",
    "    input_layer_size = 3\n",
    "    hidden_layer_size = 5\n",
    "    num_labels = 3\n",
    "    m = 5\n",
    "\n",
    "    # We generate some 'random' test data\n",
    "    Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)\n",
    "    Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)\n",
    "\n",
    "    # Reusing debugInitializeWeights to generate X\n",
    "    X = debugInitializeWeights(m, input_layer_size - 1)\n",
    "    y = np.arange(1, 1+m) % num_labels\n",
    "    # print(y)\n",
    "    # Unroll parameters\n",
    "    nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])\n",
    "\n",
    "    # short hand for cost function\n",
    "    costFunc = lambda p: nnCostFunction(p, input_layer_size, hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "    cost, grad = costFunc(nn_params)\n",
    "    numgrad = computeNumericalGradient(costFunc, nn_params)\n",
    "    \n",
    "    # Visually examine the two gradient computations.The two columns you get should be very similar.\n",
    "    print(np.stack([numgrad, grad], axis=1))\n",
    "    print('The above two columns you get should be very similar.')\n",
    "    print('(Left-Your Numerical Gradient, Right-Analytical Gradient)\\n')\n",
    "\n",
    "    # Evaluate the norm of the difference between two the solutions. If you have a correct\n",
    "    # implementation, and assuming you used e = 0.0001 in computeNumericalGradient, then diff\n",
    "    # should be less than 1e-9.\n",
    "    diff = np.linalg.norm(numgrad - grad)/np.linalg.norm(numgrad + grad)\n",
    "\n",
    "    print('If your backpropagation implementation is correct, then \\n'\n",
    "          'the relative difference will be small (less than 1e-9). \\n'\n",
    "          'Relative Difference: %g' % diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:45:35.648698Z",
     "start_time": "2019-07-26T02:45:35.616781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0841471  -0.02794155 -0.09999902 -0.02879033]\n",
      " [ 0.09092974  0.06569866 -0.05365729 -0.09613975]\n",
      " [ 0.014112    0.09893582  0.0420167  -0.07509872]\n",
      " [-0.07568025  0.04121185  0.09906074  0.01498772]\n",
      " [-0.09589243 -0.05440211  0.06502878  0.09129453]]\n",
      "---------\n",
      "[[ 0.0841471  -0.07568025  0.06569866 -0.05440211  0.0420167  -0.02879033]\n",
      " [ 0.09092974 -0.09589243  0.09893582 -0.09999902  0.09906074 -0.09613975]\n",
      " [ 0.014112   -0.02794155  0.04121185 -0.05365729  0.06502878 -0.07509872]]\n",
      "---------\n",
      "[[ 0.0841471  -0.02794155 -0.09999902]\n",
      " [ 0.09092974  0.06569866 -0.05365729]\n",
      " [ 0.014112    0.09893582  0.0420167 ]\n",
      " [-0.07568025  0.04121185  0.09906074]\n",
      " [-0.09589243 -0.05440211  0.06502878]]\n",
      "---------\n",
      "[[ -9.27825235e-03  -9.27825236e-03]\n",
      " [ -3.04978709e-06  -3.04978914e-06]\n",
      " [ -1.75060082e-04  -1.75060082e-04]\n",
      " [ -9.62660618e-05  -9.62660620e-05]\n",
      " [  8.89911959e-03   8.89911960e-03]\n",
      " [  1.42869427e-05   1.42869443e-05]\n",
      " [  2.33146358e-04   2.33146357e-04]\n",
      " [  1.17982666e-04   1.17982666e-04]\n",
      " [ -8.36010761e-03  -8.36010762e-03]\n",
      " [ -2.59383071e-05  -2.59383100e-05]\n",
      " [ -2.87468729e-04  -2.87468729e-04]\n",
      " [ -1.37149709e-04  -1.37149706e-04]\n",
      " [  7.62813551e-03   7.62813551e-03]\n",
      " [  3.69883213e-05   3.69883234e-05]\n",
      " [  3.35320347e-04   3.35320347e-04]\n",
      " [  1.53247077e-04   1.53247082e-04]\n",
      " [ -6.74798370e-03  -6.74798370e-03]\n",
      " [ -4.68759764e-05  -4.68759769e-05]\n",
      " [ -3.76215588e-04  -3.76215587e-04]\n",
      " [ -1.66560294e-04  -1.66560294e-04]\n",
      " [  3.14544970e-01   3.14544970e-01]\n",
      " [  1.64090819e-01   1.64090819e-01]\n",
      " [  1.64567932e-01   1.64567932e-01]\n",
      " [  1.58339334e-01   1.58339334e-01]\n",
      " [  1.51127527e-01   1.51127527e-01]\n",
      " [  1.49568335e-01   1.49568335e-01]\n",
      " [  1.11056588e-01   1.11056588e-01]\n",
      " [  5.75736493e-02   5.75736493e-02]\n",
      " [  5.77867378e-02   5.77867378e-02]\n",
      " [  5.59235296e-02   5.59235296e-02]\n",
      " [  5.36967009e-02   5.36967009e-02]\n",
      " [  5.31542052e-02   5.31542052e-02]\n",
      " [  9.74006970e-02   9.74006970e-02]\n",
      " [  5.04575855e-02   5.04575855e-02]\n",
      " [  5.07530173e-02   5.07530173e-02]\n",
      " [  4.91620841e-02   4.91620841e-02]\n",
      " [  4.71456249e-02   4.71456249e-02]\n",
      " [  4.65597186e-02   4.65597186e-02]]\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 2.21762e-11\n"
     ]
    }
   ],
   "source": [
    "# check gradient without regularizetion\n",
    "checkNNGradients(nnCostFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:32.422965Z",
     "start_time": "2019-07-26T02:19:32.414985Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# #  Check gradients by running checkNNGradients\n",
    "# lambda_ = 3\n",
    "# checkNNGradients(nnCostFunction, lambda_)\n",
    "\n",
    "# # Also output the costFunction debugging values\n",
    "# debug_J, _  = nnCostFunction(nn_params, input_layer_size,\n",
    "#                           hidden_layer_size, num_labels, X, y, lambda_)\n",
    "\n",
    "# print('\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' % (lambda_, debug_J))\n",
    "# print('(for lambda = 3, this value should be about 0.576051)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:32.454882Z",
     "start_time": "2019-07-26T02:19:32.426954Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, X):\n",
    "    if X.ndim == 1:\n",
    "        X = X[None]#将一维的X提升到二维\n",
    "        \n",
    "    m = X.shape[0]\n",
    "    p = np.zeros(m)\n",
    "    num_labels = Theta2.shape[0]\n",
    "    \n",
    "#   X = np.concatenate([np.ones((m, 1)), X], axis=1)  \n",
    "#     for i in range(m):\n",
    "#         a1 = X[i]\n",
    "#         a2 = np.concatenate([np.ones(1),sigmoid(np.dot(Theta1,a0))])\n",
    "#         a3 = sigmoid(np.dot(Theta2,a1))\n",
    "#         p[i] = np.argmax(a2)\n",
    "    a2 = sigmoid(np.dot(np.concatenate([np.ones((m, 1)), X], axis = 1),Theta1.T))\n",
    "    a3 = sigmoid(np.dot(np.concatenate([np.ones((m,1)),a2],axis = 1),Theta2.T))\n",
    "    p = np.argmax(a3,axis=1)\n",
    "    # p是m*1的矩阵，是样本的预测值\n",
    "    # a3是m*num_lable 的输出数组，是由sigmoid函数计算得出的，一般不为0也不为1(可以log)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:42.847110Z",
     "start_time": "2019-07-26T02:19:32.458870Z"
    }
   },
   "outputs": [],
   "source": [
    "#  After you have completed the assignment, change the maxiter to a larger\n",
    "#  value to see how more training helps.\n",
    "options= {'maxiter': 100}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 1\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params = res.x\n",
    "        \n",
    "# Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:20:33.011149Z",
     "start_time": "2019-07-26T02:20:32.978240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 94.0%\n"
     ]
    }
   ],
   "source": [
    "# use the weights trained by myself \n",
    "# it should be about 95%\n",
    "pred = predict(Theta1, Theta2, X)\n",
    "print('Training Set Accuracy: {:.1f}%'.format(np.mean(pred == y) * 100))\n",
    "#print(np.concatenate([np.ones((m, 1)), X], axis=1)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Random Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:42.962801Z",
     "start_time": "2019-07-26T02:19:42.891992Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def displayData(X, example_width=None, figsize=(10, 10)):\n",
    "    \"\"\"\n",
    "    Displays 2D data stored in X in a nice grid.\n",
    "    \"\"\"\n",
    "    # Compute rows, cols\n",
    "    if X.ndim == 2:\n",
    "        m, n = X.shape\n",
    "    elif X.ndim == 1:\n",
    "        n = X.size\n",
    "        m = 1\n",
    "        X = X[None]  # Promote to a 2 dimensional array\n",
    "    else:\n",
    "        raise IndexError('Input X should be 1 or 2 dimensional.')\n",
    "\n",
    "    example_width = example_width or int(np.round(np.sqrt(n)))\n",
    "    example_height = n / example_width\n",
    "\n",
    "    # Compute number of items to display\n",
    "    display_rows = int(np.floor(np.sqrt(m)))\n",
    "    display_cols = int(np.ceil(m / display_rows))\n",
    "\n",
    "    fig, ax_array = pyplot.subplots(display_rows, display_cols, figsize=figsize)\n",
    "    fig.subplots_adjust(wspace=0.025, hspace=0.025)\n",
    "\n",
    "    ax_array = [ax_array] if m == 1 else ax_array.ravel()\n",
    "\n",
    "    for i, ax in enumerate(ax_array):\n",
    "        ax.imshow(X[i].reshape(example_width, example_width, order='F'),\n",
    "                  cmap='Greys', extent=[0, 1, 0, 1])\n",
    "        ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:19:43.120382Z",
     "start_time": "2019-07-26T02:19:42.968813Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Prediction: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ed41695198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# randomly permute examples, to be used for visualizing one \n",
    "# picture at a time\n",
    "indices = np.random.permutation(m)\n",
    "\n",
    "# Randomly select 100 data points to display\n",
    "rand_indices = np.random.choice(m, 100, replace=False)\n",
    "sel = X[rand_indices, :]\n",
    "\n",
    "if indices.size > 0:\n",
    "    i, indices = indices[0], indices[1:]\n",
    "    displayData(X[i, :], figsize=(4, 4))\n",
    "    pred = predict(Theta1, Theta2, X[i, :])\n",
    "    print('Neural Network Prediction: {}'.format(*pred))\n",
    "else:\n",
    "    print('No more images to display!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
